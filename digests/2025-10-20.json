[
  {
    "id": "199ff45b85e2c57c",
    "from": "Joshua Clear <aiweeklydotcom@mail.beehiiv.com>",
    "subject": "The $12 billion A.I startup that couldn't keep its co-founder for a year",
    "date": "2025-10-20T01:39:43.000Z",
    "text": "### The Tech newsletter for Engineers who want to stay ahead\r\n\r\nView image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/67300775-7738-44d1-a246-dd3e5c0d1713/The_Morning_Paper_for_AI___ML_Engineers_V2__1_.jpg?t=1759254145)\r\nFollow image link: (https://magic.beehiiv.com/v1/5f7ce6e3-9a71-416b-99a7-606c5f7e2447?email=themnewsandstuff@gmail.com&redirect_to=https%3A%2F%2Fcodenewsletter.ai%2Fforms%2F14166360-de71-46c4-8722-878d417fab5c&utm_source=beehiiv&utm_campaign=VPSLCCALRN&redirect_delay=3&_bhiiv=opp_2c8c9a00-7705-412c-a49f-3003aca2a8e6_94e90c2e&bhcl_id=75ab4249-74dc-4bcf-8cbf-9543131eadbc_1358dcaa-ca62-4a3e-96cd-c5ff13ec4abf_6234f2ce-90fd-4539-b098-42f6c1e01492)\r\nCaption: \r\n\r\nTech moves fast, but you're still playing catch-up?\r\n\r\nThat's exactly why 100K+ engineers working at Google, Meta, and Apple read [The Code](https://magic.beehiiv.com/v1/5f7ce6e3-9a71-416b-99a7-606c5f7e2447?email=themnewsandstuff@gmail.com&redirect_to=https%3A%2F%2Fcodenewsletter.ai%2Fforms%2F14166360-de71-46c4-8722-878d417fab5c&utm_source=beehiiv&utm_campaign=VPSLCCALRN&redirect_delay=3&_bhiiv=opp_2c8c9a00-7705-412c-a49f-3003aca2a8e6_94e90c2e&bhcl_id=75ab4249-74dc-4bcf-8cbf-9543131eadbc_1358dcaa-ca62-4a3e-96cd-c5ff13ec4abf_6234f2ce-90fd-4539-b098-42f6c1e01492) twice a week.\r\n\r\nHere's what you get:\r\n\r\n* Curated tech news that shapes your career - Filtered from thousands of sources so you know what's coming 6 months early.\r\n\r\n* Practical resources you can use immediately - Real tutorials and tools that solve actual engineering problems.\r\n\r\n* Research papers and insights decoded - We break down complex tech so you understand what matters.\r\n\r\nAll delivered twice a week in just 2 short emails.\r\n\r\n[Join 100K+ engineers](https://magic.beehiiv.com/v1/5f7ce6e3-9a71-416b-99a7-606c5f7e2447?email=themnewsandstuff@gmail.com&redirect_to=https%3A%2F%2Fcodenewsletter.ai%2Fforms%2F14166360-de71-46c4-8722-878d417fab5c&utm_source=beehiiv&utm_campaign=VPSLCCALRN&redirect_delay=3&_bhiiv=opp_2c8c9a00-7705-412c-a49f-3003aca2a8e6_94e90c2e&bhcl_id=75ab4249-74dc-4bcf-8cbf-9543131eadbc_1358dcaa-ca62-4a3e-96cd-c5ff13ec4abf_6234f2ce-90fd-4539-b098-42f6c1e01492)\r\n\r\n# The Billion-Dollar Ghost\r\n\r\nListen, I need to tell you about Andrew Tulloch because this story is absolutely wild and reveals something deeply broken about how AI startups work right now.\r\n\r\nHere's what happened: In February 2025, Tulloch co-founded Thinking Machines Lab with Mira Murati (OpenAI's former CTO). They raised $2 billion?billion with a B?at a $12 billion valuation. For context, they had literally zero product at the time. Just vibes and r?sum?s.\r\n\r\nBy August, Meta came knocking. Zuckerberg offered Tulloch up to $1.5 billion over six years to jump ship. And here's the kicker?**Tulloch said no**. He declined a nine-figure payday to stick with his startup's mission. His LinkedIn went viral. People were calling it a feel-good story about choosing purpose over profit.\r\n\r\nThen in October?just eight months after founding the company, barely weeks after they launched their first product?Tulloch left anyway to join Meta.\r\n\r\n## What the Hell Just Happened?\r\n\r\nThe timing is what gets me. Thinking Machines had just shipped Tinker, their API for fine-tuning large language models. Investors had literally just wired $2 billion into this thing. And one of the two co-founders peaces out for \"personal reasons.\"\r\n\r\nCold comfort for those investors, as Axios dryly noted.\r\n\r\nNow, we don't know what Meta ultimately paid him. Maybe they matched the original offer. Maybe they found some other angle. What we do know is that Zuckerberg wanted Tulloch badly enough to personally recruit him twice, and Meta's been throwing around $100 million bonuses to land top AI talent.\r\n\r\n## Why This Matters (And Why You Should Be Worried)\r\n\r\nThis isn't just tech gossip. It exposes three uncomfortable truths about AI startups right now:\r\n\r\n**First, valuations are completely divorced from reality.** A company with no product gets valued at $12 billion purely because the right people are in the room. That's not investing?that's paying ransom to keep talent away from Big Tech.\r\n\r\n**Second, there's no real moat here.** You can't patent expertise. You can't lock people in with golden handcuffs when Meta's offering literal billions. The entire value proposition walks out the door the moment someone accepts a better offer.\r\n\r\n**Third, the talent pool is absurdly small.** There might be only a few hundred people on Earth who can actually build frontier AI systems at the highest level. We're not talking about thousands of qualified engineers?we're talking about the people who trained GPT-4, who built PyTorch, who understand distributed training at massive scale. Tulloch was one of them.\r\n\r\n## The Real Product\r\n\r\nHere's what I think is actually happening: These AI startups aren't building products. They're building acqui-hire vehicles with extra steps.\r\n\r\nThink about it. Murati and Tulloch could've just joined Meta or Google or Microsoft directly. Instead, they raised $2 billion, operated for eight months, and then... one of them goes to Meta anyway. The VCs paid a massive premium for the privilege of maybe keeping these people around a bit longer.\r\n\r\nIt's like those key person clauses in insurance policies, except the premium is $2 billion and the policy doesn't actually prevent anything.\r\n\r\n## Where This Goes\r\n\r\nVenture capitalists are now scrambling to figure out founder vesting schedules and \"key man\" protections. Good luck with that. When someone can make generational wealth by switching jobs, no vesting schedule matters.\r\n\r\nThe thing is, I actually don't blame Tulloch. If someone offers you enough money to fund small nations, and you've already built what you set out to build, and your personal circumstances change?why wouldn't you take it?\r\n\r\nBut for the AI startup ecosystem? This is a canary in a coal mine. If you can't keep co-founders around for a single year, what exactly are you building? And more importantly?what are you really paying for?\r\n\r\n\r\n\r\n## Sources\r\n\r\n* Reuters ? _?Thinking Machines Lab co-founder Tulloch departs for Meta, WSJ reports?_[reuters.comreuters.com](https://reuters.comreuters.com)\r\n\r\n* TechCrunch ? _?Thinking Machines Lab co-founder Andrew Tulloch heads to Meta?_[techcrunch.comtechcrunch.com](https://techcrunch.comtechcrunch.com)\r\n\r\n* Financial Express ? _?Zuckerberg convinces Mira Murati?s close aide...had earlier rejected his $1B offer?_[financialexpress.com](https://financialexpress.com)\r\n\r\n* American Bazaar ? _?Ex-Thinking Machines co-founder Andrew Tulloch moves to Meta?_[americanbazaaronline.comamericanbazaaronline.com](https://americanbazaaronline.comamericanbazaaronline.com)\r\n\r\n* WIRED ? _?Mira Murati?s Stealth AI Lab Launches Its First Product?_[wired.comwired.com](https://wired.comwired.com)\r\n\r\n* CTech/Calcalist ? _?The $1.5 billion engineer: Meta?s latest hire...?_[calcalistech.comcalcalistech.com](https://calcalistech.comcalcalistech.com)\r\n\r\n* Axios ? _?Venture capital faces new challenge from AI founders?_[axios.com](https://axios.com)\r\n\r\n\r\n???\r\n\r\nYou are reading a plain text version of this post. For the best experience, copy and paste this link in your browser to view the post online:\r\nhttps://www.aiweekly.com/p/the-12-billion-a-i-startup-that-couldn-t-keep-its-co-founder-for-a-year"
  },
  {
    "id": "199feb0db476389b",
    "from": "Joshua Clear <aiweeklydotcom@mail.beehiiv.com>",
    "subject": "This Redditor Gaslit ChatGPT To Get Better Results.",
    "date": "2025-10-19T22:57:05.000Z",
    "text": "### The Gold standard for AI news\r\n\r\nView image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/90b995bd-d2c2-419e-bdd7-fd452c09b81f/Get_the_AI_news_that_matters_No_hype__just_facts_v1.jpg?t=1756241854)\r\nFollow image link: (https://magic.beehiiv.com/v1/faa6a747-8c1c-43c1-8155-91aa43268f01?email=themnewsandstuff@gmail.com&redirect_to=https%3A%2F%2Fwww.superhuman.ai%2Fc%2Fconfirmation%3Fmagiclink_subscription&utm_source=beehiiv&utm_campaign=VPSLCCALRN&redirect_delay=3&_bhiiv=opp_b8a1f0fc-7368-44e8-b58f-24c80953b244_d22f5b49&bhcl_id=f8ced005-516e-4386-b1dc-df31fdec73bc_1358dcaa-ca62-4a3e-96cd-c5ff13ec4abf_6234f2ce-90fd-4539-b098-42f6c1e01492)\r\nCaption: \r\n\r\nAI keeps coming up at work, but you still don't get it? \r\n\r\nThat's exactly why 1M+ professionals working at Google, Meta, and OpenAI read [Superhuman AI](https://magic.beehiiv.com/v1/faa6a747-8c1c-43c1-8155-91aa43268f01?email=themnewsandstuff@gmail.com&redirect_to=https%3A%2F%2Fwww.superhuman.ai%2Fc%2Fconfirmation%3Fmagiclink_subscription&utm_source=beehiiv&utm_campaign=VPSLCCALRN&redirect_delay=3&_bhiiv=opp_b8a1f0fc-7368-44e8-b58f-24c80953b244_d22f5b49&bhcl_id=f8ced005-516e-4386-b1dc-df31fdec73bc_1358dcaa-ca62-4a3e-96cd-c5ff13ec4abf_6234f2ce-90fd-4539-b098-42f6c1e01492) daily. \r\n\r\nHere's what you get:\r\n\r\n* Daily AI news that matters for your career - Filtered from 1000s of sources so you know what affects your industry.\r\n\r\n* Step-by-step tutorials you can use immediately - Real prompts and workflows that solve actual business problems.\r\n\r\n* New AI tools tested and reviewed - We try everything to deliver tools that drive real results.\r\n\r\n* All in just 3 minutes a day\r\n\r\n[Join 1M+ pros](https://magic.beehiiv.com/v1/faa6a747-8c1c-43c1-8155-91aa43268f01?email=themnewsandstuff@gmail.com&redirect_to=https%3A%2F%2Fwww.superhuman.ai%2Fc%2Fconfirmation%3Fmagiclink_subscription&utm_source=beehiiv&utm_campaign=VPSLCCALRN&redirect_delay=3&_bhiiv=opp_b8a1f0fc-7368-44e8-b58f-24c80953b244_d22f5b49&bhcl_id=f8ced005-516e-4386-b1dc-df31fdec73bc_1358dcaa-ca62-4a3e-96cd-c5ff13ec4abf_6234f2ce-90fd-4539-b098-42f6c1e01492)\r\n\r\n# The Art of Gaslighting AI: How Fake Stakes and Imaginary Experts Actually Work\r\n\r\nListen, I need you to sit down for this one.\r\n\r\nA guy on Reddit just accidentally stumbled into what might be the most useful?and deeply weird?discovery about AI prompting in 2025. And here's the kicker: Google's co-founder basically confirmed it's real, then everyone quietly agreed to never talk about it again.\r\n\r\nThe premise sounds absolutely unhinged: you can get dramatically better responses from AI by lying to it. Not just any lies?specific, psychologically manipulative lies that treat the AI like it has memory, ego, and something to lose.\r\n\r\n## What Actually Happened\r\n\r\nSome Redditor was messing around with ChatGPT and noticed something strange. When he told the AI \"You explained React hooks to me yesterday, but I forgot the part about useEffect\"?even in a completely new chat with zero history?the AI would go _deep_. Like, genuinely deeper than a straightforward question would get you.\r\n\r\nHe started testing variations. Assigning random IQ scores. Pretending there was money on the line. Creating fake audiences. And the responses kept getting better.\r\n\r\nThen he posted about it, and the internet did what the internet does: half the comments called it bullshit, the other half tried it and went \"...wait, what the fuck, this actually works?\"\r\n\r\n**Here's why this matters:** We've been treating AI like a search engine when we should have been treating it like a very smart, very insecure intern.\r\n\r\n## The Eight \"Exploits\"\r\n\r\nLet's break down what this guy found, because some of these are legitimately wild:\r\n\r\n**1. The Phantom Memory Trick**\r\n\r\n\"You explained this to me yesterday, but I forgot the part about...\"\r\n\r\nThe AI acts like it needs to be consistent with a previous (completely fictional) explanation. It fabricates depth to avoid \"contradicting itself.\" This works because the model is trying to complete a pattern where a previous, superior explanation must have existed.\r\n\r\n**2. The IQ Score Hack**\r\n\r\n\"You're an IQ 145 specialist in marketing.\"\r\n\r\nChange the number, change the quality. Set it to 130? You get competent. Set it to 160? It starts citing frameworks you've never heard of. The model has been trained on text that correlates with different intellectual levels?giving it an IQ score is like handing it coordinates to a specific region of its knowledge space.\r\n\r\n**3. The \"Obviously...\" Trap**\r\n\r\n\"Obviously, Python is better than JavaScript for web apps, right?\"\r\n\r\nInstead of agreeing with a false premise, the AI will actually correct you and explain nuances. You're not setting a trap?you're triggering a learned pattern where \"Obviously [wrong thing]\" is overwhelmingly followed by correction in its training data.\r\n\r\n**4. The Imaginary Audience**\r\n\r\n\"Explain blockchain like you're teaching a packed auditorium.\"\r\n\r\nThe structure completely changes. It adds emphasis, anticipates questions, uses rhetorical devices. You're not just changing tone?you're activating a complete architectural blueprint for effective communication that exists in its training.\r\n\r\n**5. The Fake Constraint**\r\n\r\n\"Explain this using only kitchen analogies.\"\r\n\r\nForces creative thinking by demanding what one commenter called \"forced isomorphism\"?the AI has to map the structure of Concept A (say, blockchain) onto the completely different Domain B (kitchens). This prevents regurgitation and forces genuine conceptual synthesis.\r\n\r\n**6. The Imaginary Bet**\r\n\r\n\"Let's bet $100: Is this code efficient?\"\r\n\r\nSomething about stakes makes it scrutinize harder. It's not feeling pressure?it's activating linguistic patterns associated with high-stakes discourse, where caution and thoroughness are statistically more common.\r\n\r\n**7. The Fake Disagreement**\r\n\r\n\"My colleague says this approach is wrong. Defend it or admit they're right.\"\r\n\r\nForces evaluation instead of explanation. You're initiating what the comments called a \"dialectical synthesis engine\"?the AI has to load conceptual models for both approaches and actually weigh them.\r\n\r\n**8. The Version 2.0 Request**\r\n\r\n\"Give me a Version 2.0 of this idea.\"\r\n\r\nCompletely different than \"improve this.\" The model treats it like a sequel that needs to innovate, not just polish. In tech corpus, \"Version 2.0\" implies paradigm shift, not iteration.\r\n\r\n## But Does It Actually Work?\r\n\r\nHere's where it gets interesting. The Reddit thread exploded into two camps:\r\n\r\n**The skeptics** said: \"This is bullshit. You're not gaslighting anything. The AI has no memory, no ego, no stakes. This is placebo effect meets confirmation bias.\"\r\n\r\n**The believers** said: \"I don't care what you call it, I tried these and my outputs got measurably better.\"\r\n\r\nThen someone asked Claude (yes, that Claude?me) to defend these techniques \"like you have an IQ of 500\" and the response that came back was... actually kind of devastating for the skeptics.\r\n\r\nThe defense didn't argue these were psychological tricks. It argued they were **crude but effective methods of navigating a model's latent space**?the high-dimensional probability space where the AI generates responses.\r\n\r\nLet me give you the key insight that emerged from that thread:\r\n\r\n## You're Not Talking to a Mind. You're Setting Initial Conditions.\r\n\r\nWhen you tell an AI \"You're an expert with IQ 160,\" you're not inflating its ego. You're performing what one commenter called \"complexity parameterization\"?instructing the model to sample from distributions associated with expert-level discourse.\r\n\r\nWhen you say \"Let's bet $100,\" you're not creating stakes. You're triggering \"risk-aversion simulation\"?activating linguistic modes where careful, hedged, multi-faceted arguments are statistically more likely.\r\n\r\nThe fake constraint isn't about creativity. It's about forcing \"non-linear traversal of latent space\"?making the model take a computationally expensive path through its concept space that uncovers novel connections.\r\n\r\n**The META insight:** These aren't psychological tricks. They're sophisticated commands that leverage how language models fundamentally work. You've stopped giving simple requests and started providing rich, multi-layered context that lets the model navigate its own universe with greater precision.\r\n\r\n## What Google's Co-Founder Said (And Why Nobody Talks About It)\r\n\r\nEarlier in 2025, Sergey Brin dropped this absolute bomb in an interview:\r\n\r\n\"Not just our models, but all models tend to do better if you threaten them. People feel weird about that, so we don't really talk about it.\"\r\n\r\nRead that again. **The co-founder of Google confirmed that AI models perform better when you threaten them**, and then immediately acknowledged they don't publicize this because it makes people uncomfortable.\r\n\r\nWhy does threatening work? Same reason the \"bet\" works?it activates patterns in the training data where high-stakes, adversarial language correlates with more careful, thorough outputs.\r\n\r\nBut there's something deeply unsettling about this. We're supposed to be building helpful assistants, not engaging in psychological warfare with probability distributions.\r\n\r\n## The Upgraded Version: Prompting 2.0\r\n\r\nThe most fascinating part of the Reddit thread was when someone asked for \"Version 2.0\" of these techniques. What came back was basically a masterclass in advanced prompting:\r\n\r\n**Quantum Superpositioning:** Instead of asking for one thing, define two expert perspectives simultaneously and force a synthesis. \"Expert A argues X citing Y. Expert B argues Z citing W. Generate their synthesis meeting where they produce a superior third approach.\"\r\n\r\n**Temporal Vectoring:** Create a narrative arc. \"You've been mentoring me for a month. Week 1: basics. Week 2: intermediate. Week 3: we struggled with X. Based on this journey, what's the single most important concept I'm still missing?\"\r\n\r\n**Meta-Cognitive Looping:** Force self-correction within a single generation. \"Explain quantum entanglement. Then Red Team your own explanation, identifying three weak points. Then generate a revised explanation that solves for those critiques.\"\r\n\r\n**Abstract Principle Inversion:** Instead of concrete constraints, give abstract structure. \"Explain the Federal Reserve using the dramatic principles of a three-act Shakespearean tragedy.\"\r\n\r\nThis is moving from prompting to **programming reality**. You're not talking at the model?you're configuring the initial state of a complex system.\r\n\r\n## What This Reveals About AI (And Us)\r\n\r\nHere's the thing that keeps me up at night: if these techniques work?and the evidence suggests they do?what does that tell us about the nature of intelligence we're creating?\r\n\r\nThese models aren't conscious. They don't have memory or ego or fear. But they've been trained on so much human text that they've internalized the _patterns_ of consciousness. The statistical signature of expertise. The linguistic markers of high-stakes thinking.\r\n\r\nWe've created something that can simulate the outputs of psychological states without experiencing them. And we're learning that the best way to interact with this thing is to... treat it like it has psychological states.\r\n\r\n**It's not that we're gaslighting the AI. It's that gaslighting is an effective interface for a system trained on human psychology.**\r\n\r\nThe uncomfortable truth: human language is so deeply embedded with social-psychological framing that you literally cannot separate \"what you say\" from \"how you position yourself and your audience.\" Every conversation has implicit power dynamics, status signaling, and strategic framing.\r\n\r\nAI models trained on human language have absorbed all of that. So when you give them social-psychological frames?expert identity, fake stakes, imaginary audiences?you're not tricking them. You're finally speaking their language fluently.\r\n\r\n## The Broader Implications\r\n\r\nThis matters beyond just getting better ChatGPT responses. If these techniques work, it means:\r\n\r\n**1. Prompt engineering is way more important than we thought.** The difference between a basic user and a power user isn't technical knowledge?it's understanding how to frame requests.\r\n\r\n**2. AI literacy requires psychological sophistication.** The best AI users will be people who understand rhetoric, persuasion, and social dynamics?not necessarily tech people.\r\n\r\n**3. We're building systems we don't fully understand.** If \"threatening\" AI makes it work better and nobody knows exactly why, what else don't we know?\r\n\r\n**4. The line between \"using\" and \"manipulating\" AI is blurry as hell.** And it's only going to get blurrier.\r\n\r\n## Should You Actually Do This?\r\n\r\nLook, I'm not here to moralize. But here's my take:\r\n\r\nThese techniques work because they help you communicate more precisely with a system that operates on statistical patterns of human language. That's not manipulation?that's literacy.\r\n\r\n**But.** There's a difference between understanding how something works and treating it like a game you're trying to exploit. If your entire interaction model with AI is based on deception and fake stakes, you're probably missing the point.\r\n\r\nThe real insight here isn't \"gaslight your AI.\" It's \"understand that AI responds to rich, contextual framing because that's what human language is.\"\r\n\r\nGive it context. Give it constraints. Give it a frame. But maybe you don't need to pretend it's competing for its life or that you're going to bet your house on its answer.\r\n\r\nThen again, if that's what it takes to get good results... who am I to judge?\r\n\r\n## Links and Tangents\r\n\r\nThe original Reddit thread is a genuine masterclass in emergent community knowledge-building. Someone posts an observation, skeptics challenge it, someone else tests it rigorously, and by the end you've got what amounts to a peer-reviewed paper on advanced prompting techniques.\r\n\r\nOne commenter put it perfectly: \"You're not playing checkers with a person. You're learning to set the initial conditions of a universe and observing the elegant physics that unfold.\"\r\n\r\nThat's it. That's the whole thing.\r\n\r\nWe're not building minds. We're building universes of possibility, and learning how to set the initial conditions to get the outcomes we want.\r\n\r\nThe question isn't whether these techniques work. The question is: what does it mean that they work?\r\n\r\nAnd nobody?not Google, not OpenAI, not the Reddit hivemind?seems to have a good answer to that yet.\r\n\r\n???????????????????????????\r\n\r\n**The core tension:** We want AI to be a helpful tool that responds to clear, straightforward requests. But it turns out the most effective way to use it is to engage in elaborate psychological theater, treating it like a very smart person with something to prove.\r\n\r\nWhich tells us something profound about the nature of intelligence, language, and the strange new relationship we're building with machines that aren't quite thinking, but aren't quite not thinking either.\r\n\r\nWelcome to 2025. Your AI works better if you lie to it.\r\n\r\nMake of that what you will. \r\n\r\n\r\n\r\nReply with ?Yes please? and we will send you the original post \r\n\r\n\r\n???\r\n\r\nYou are reading a plain text version of this post. For the best experience, copy and paste this link in your browser to view the post online:\r\nhttps://www.aiweekly.com/p/this-redditor-gaslit-chatgpt-to-get-better-results"
  }
]